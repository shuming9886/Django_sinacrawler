# python3
# coding=utf-8
import requests
import json
from bs4 import BeautifulSoup
from urllib import parse
import re

import win_unicode_console
win_unicode_console.enable()

headers = {'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'}

url="https://m.weibo.cn/container/getIndex?type=wb&queryVal={}&luicode=10000011&lfid=106003type%3D1&title={}&containerid=100103type%3D2%26q%3D{}&page={}"

def crawler(keyword):
    result=[]
    pagecode=parse.quote(keyword)
    # çˆ¬å–å¾®åšçš„é¡µé¢æ•°é‡
    pagenum=10
    fw=open("weibo.txt","w",encoding='utf-8')
    for i in range(1,pagenum):
        res = requests.get(url.format(pagecode, pagecode, pagecode, i))
        content = str(res.content, encoding="utf-8")
        thisdata = json.loads(content)      #strè½¬dict
        for i in range(len(thisdata['data']['cards'][0]['card_group'])):
            # itemID = thisdata['data']['cards'][0]['card_group'][i]['mblog']['id']
            # itemCreate = thisdata['data']['cards'][0]['card_group'][i]['mblog']['created_at']
            # itemUser = thisdata['data']['cards'][0]['card_group'][i]['mblog']['user']['screen_name']
            itemText = thisdata['data']['cards'][0]['card_group'][i]['mblog']['text']
            if thisdata['data']['cards'][0]['card_group'][i]['mblog']['isLongText'] == True:
                itemText =thisdata['data']['cards'][0]['card_group'][i]['mblog']['longText']['longTextContent']
            soup = BeautifulSoup(itemText, "html.parser")
            itemTextPretty = ""
            for string in soup.stripped_strings:
                itemTextPretty += string
            pattern=re.compile('\n+')                                   #å»é™¤æ–‡æœ¬ä¸­çš„æ¢è¡Œç¬¦
            itemTextPretty=pattern.sub('',itemTextPretty)
            pattern1=re.compile(r'http://[a-zA-Z0-9.?/&=:]*', re.S)     #å»é™¤æ–‡æœ¬ä¸­çš„urlé“¾æ¥
            itemTextPretty=pattern1.sub('',itemTextPretty)
            # pattern2=re.compile('ğŸ‘‰|ğŸ™|ğŸ‘|ğŸ˜|ğŸ˜Š|ğŸ’°|ğŸ|ğŸ¤”|ğŸŒ‚|ğŸ™Š|âœŒ|ğŸ‘¿|ğŸ¬|ğŸ’“|ğŸ™|â†“|â™¡|ğŸ’ª|ğŸº|ğŸŒŸ',re.S)         #å»é™¤æ–‡æœ¬ä¸­çš„è¡¨æƒ…ç¬¦å·
            # itemTextPretty=pattern2.sub('',itemTextPretty)
            pattern3=re.compile(r'\[.*?\]',re.S)                         #å»é™¤æ–‡æœ¬ä¸­çš„è¡¨æƒ…[æ˜Ÿæ˜Ÿ][å¿ƒ]
            itemTextPretty=pattern3.sub('',itemTextPretty)
            pattern4=re.compile(r'ç½‘é¡µé“¾æ¥',re.S)
            itemTextPretty=pattern4.sub('',itemTextPretty)
            pattern5=re.compile(r'(#\w+#)',re.S)                         #å»é™¤ä¸»é¢˜
            itemTextPretty=pattern5.sub('',itemTextPretty)
            pattern6=re.compile(r'(@\w+)',re.S)                          #å»é™¤@
            itemTextPretty=pattern6.sub('',itemTextPretty)
            pattern7=re.compile(r'//:',re.S)
            itemTextPretty=pattern7.sub('',itemTextPretty)
            pattern8=re.compile(r'è½¬å‘å¾®åš',re.S)
            itemTextPretty=pattern8.sub('',itemTextPretty)
            itemTextPretty.strip()
            if itemTextPretty!='' and itemTextPretty not in result:
                result.append(itemTextPretty)
                fw.write(itemTextPretty+'\n')
    fw.close()
    return result

def main():
    weibodata=crawler("è‹å·å¤§å­¦")
    for data in weibodata:
        print(data)

if __name__ == '__main__':
    main()